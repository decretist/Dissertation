{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burrows's Delta (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_tokens(filename):\n",
    "    '''open text file and return list of tokens'''\n",
    "    # text = open(filename, 'r').read().lower()\n",
    "    f = open(filename, 'r') # open file\n",
    "    text = f.read() # read file\n",
    "    text = text.lower() # lower-case text\n",
    "    tokens = [word for word in re.split('\\W', text) if word != ''] # remove punctuation\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(samples):\n",
    "    tokens = []\n",
    "    for sample in samples:\n",
    "        tokens += get_tokens(path + sample + '.txt')\n",
    "    types = list(set(tokens)) # create unordered list of unique words\n",
    "    tmp = dict.fromkeys(types, 0) # create temporary dictionary, initialize counts to 0\n",
    "    for token in tokens: tmp[token] += 1 # count words\n",
    "    # re-order words in temporary dictionary numerically by descending frequency\n",
    "    # re-order words with same frequency alphabetically\n",
    "    features = { \n",
    "        key: value for key, value in sorted(tmp.items(),\n",
    "        key = lambda item: (-item[1], item[0]))\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_counts(features, samples):\n",
    "    columns = {}\n",
    "    for sample in samples:\n",
    "        columns[sample] = []\n",
    "        tmp = get_features([sample])\n",
    "        for feature in features:\n",
    "            columns[sample].append(tmp.get(feature, 0))\n",
    "    return pd.DataFrame(columns, index = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(samples):\n",
    "    filenames = [path + sample + '.txt' for sample in samples]\n",
    "    lengths = {}\n",
    "    for i in range(len(samples)):\n",
    "       lengths[samples[i]] = len(get_tokens(filenames[i]))\n",
    "    return pd.DataFrame(lengths, index = ['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "['sed', 'unde', 'enim', 'ait', 'ergo']\n",
      "['in', 'non', 'et', 'est', 'de', 'quod', 'qui', 'ad', 'uel', 'ut', 'si', 'autem', 'cum', 'a', 'ex', 'sunt', 'que', 'etiam', 'uero', 'ab', 'quia', 'esse', 'item', 'per', 'nec', 'se', 'hoc', 'nisi', 'ita', 'illud']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gratian1</th>\n",
       "      <th>dePen</th>\n",
       "      <th>Gratian2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gratian0</th>\n",
       "      <td>3.463527</td>\n",
       "      <td>3.477218</td>\n",
       "      <td>3.380142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Gratian1     dePen  Gratian2\n",
       "Gratian0  3.463527  3.477218  3.380142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "path = './corpus/a/'\n",
    "limit = 30 # 30 most frequent words (MFWs)\n",
    "samples = ['Gratian1', 'dePen', 'Gratian2']\n",
    "unknown = 'Gratian0'\n",
    "# unknown = 'psAug'\n",
    "samples_features = get_features(samples)\n",
    "unknown_features = get_features([unknown])\n",
    "missing_features = [word for word in list(samples_features.keys())[:limit] if word not in unknown_features]\n",
    "print(missing_features, file = sys.stderr)\n",
    "features = [word for word in samples_features if word in unknown_features]\n",
    "mfws = features[:limit]\n",
    "print(mfws, file = sys.stderr)\n",
    "counts = get_counts(mfws, [unknown] + samples)\n",
    "lengths = get_lengths([unknown] + samples)\n",
    "frequencies = (counts / lengths.values) * 1000\n",
    "means = frequencies[samples].mean(axis = 1).to_frame('mean')\n",
    "standard_deviations = frequencies[samples].std(axis = 1).to_frame('std')\n",
    "z_scores = (frequencies - means.values) / standard_deviations.values\n",
    "test = z_scores[[unknown]]\n",
    "corpus = z_scores[samples]\n",
    "differences = (test.values - corpus).abs()\n",
    "row = (differences.mean(axis = 0)).to_frame(unknown).transpose()\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cases</th>\n",
       "      <th>laws</th>\n",
       "      <th>orders1</th>\n",
       "      <th>orders2</th>\n",
       "      <th>simony</th>\n",
       "      <th>procedure</th>\n",
       "      <th>other1</th>\n",
       "      <th>other2</th>\n",
       "      <th>monastic</th>\n",
       "      <th>other3</th>\n",
       "      <th>heresy</th>\n",
       "      <th>marriage</th>\n",
       "      <th>penance</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cases</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.276541</td>\n",
       "      <td>1.924653</td>\n",
       "      <td>2.025227</td>\n",
       "      <td>1.963736</td>\n",
       "      <td>1.954481</td>\n",
       "      <td>1.571380</td>\n",
       "      <td>2.278196</td>\n",
       "      <td>1.762169</td>\n",
       "      <td>2.362842</td>\n",
       "      <td>1.871684</td>\n",
       "      <td>1.892278</td>\n",
       "      <td>1.858944</td>\n",
       "      <td>1.633416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laws</th>\n",
       "      <td>2.140962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.248994</td>\n",
       "      <td>1.502025</td>\n",
       "      <td>1.463297</td>\n",
       "      <td>1.314724</td>\n",
       "      <td>1.422318</td>\n",
       "      <td>1.436900</td>\n",
       "      <td>1.193099</td>\n",
       "      <td>1.434469</td>\n",
       "      <td>1.187540</td>\n",
       "      <td>1.192357</td>\n",
       "      <td>1.621796</td>\n",
       "      <td>1.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orders1</th>\n",
       "      <td>1.618391</td>\n",
       "      <td>1.094915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.122318</td>\n",
       "      <td>0.968477</td>\n",
       "      <td>0.884251</td>\n",
       "      <td>1.049854</td>\n",
       "      <td>1.110873</td>\n",
       "      <td>0.869308</td>\n",
       "      <td>1.239660</td>\n",
       "      <td>0.826694</td>\n",
       "      <td>1.012354</td>\n",
       "      <td>0.750538</td>\n",
       "      <td>0.777653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orders2</th>\n",
       "      <td>1.898201</td>\n",
       "      <td>1.524410</td>\n",
       "      <td>1.268620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.382037</td>\n",
       "      <td>1.683955</td>\n",
       "      <td>1.414852</td>\n",
       "      <td>1.687281</td>\n",
       "      <td>1.449174</td>\n",
       "      <td>1.620788</td>\n",
       "      <td>1.419841</td>\n",
       "      <td>1.452577</td>\n",
       "      <td>1.552287</td>\n",
       "      <td>1.319530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simony</th>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.349079</td>\n",
       "      <td>0.977183</td>\n",
       "      <td>1.219516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.887776</td>\n",
       "      <td>1.130389</td>\n",
       "      <td>1.128661</td>\n",
       "      <td>1.041277</td>\n",
       "      <td>1.171096</td>\n",
       "      <td>0.590034</td>\n",
       "      <td>0.916648</td>\n",
       "      <td>0.905909</td>\n",
       "      <td>1.086345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procedure</th>\n",
       "      <td>1.618734</td>\n",
       "      <td>1.199074</td>\n",
       "      <td>0.892039</td>\n",
       "      <td>1.509545</td>\n",
       "      <td>0.878923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.079020</td>\n",
       "      <td>1.122308</td>\n",
       "      <td>0.820975</td>\n",
       "      <td>1.072558</td>\n",
       "      <td>0.656892</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.881784</td>\n",
       "      <td>0.985186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other1</th>\n",
       "      <td>1.335318</td>\n",
       "      <td>1.300002</td>\n",
       "      <td>1.061945</td>\n",
       "      <td>1.272178</td>\n",
       "      <td>1.138310</td>\n",
       "      <td>1.075322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.279177</td>\n",
       "      <td>0.964928</td>\n",
       "      <td>1.305428</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>1.085321</td>\n",
       "      <td>1.327212</td>\n",
       "      <td>0.815153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other2</th>\n",
       "      <td>1.941600</td>\n",
       "      <td>1.323324</td>\n",
       "      <td>1.091294</td>\n",
       "      <td>1.629055</td>\n",
       "      <td>1.138646</td>\n",
       "      <td>1.109048</td>\n",
       "      <td>1.296338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>1.034580</td>\n",
       "      <td>1.059174</td>\n",
       "      <td>0.653951</td>\n",
       "      <td>0.863307</td>\n",
       "      <td>1.096104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monastic</th>\n",
       "      <td>1.455489</td>\n",
       "      <td>1.045089</td>\n",
       "      <td>0.855446</td>\n",
       "      <td>1.267636</td>\n",
       "      <td>1.011384</td>\n",
       "      <td>0.798639</td>\n",
       "      <td>0.930010</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.057799</td>\n",
       "      <td>0.760244</td>\n",
       "      <td>0.661069</td>\n",
       "      <td>0.799881</td>\n",
       "      <td>0.779872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other3</th>\n",
       "      <td>2.070457</td>\n",
       "      <td>1.338796</td>\n",
       "      <td>1.288958</td>\n",
       "      <td>1.514642</td>\n",
       "      <td>1.199653</td>\n",
       "      <td>1.105702</td>\n",
       "      <td>1.349715</td>\n",
       "      <td>0.950503</td>\n",
       "      <td>1.122944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.120868</td>\n",
       "      <td>0.712106</td>\n",
       "      <td>1.152123</td>\n",
       "      <td>1.306677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heresy</th>\n",
       "      <td>1.517690</td>\n",
       "      <td>1.030980</td>\n",
       "      <td>0.777157</td>\n",
       "      <td>1.218226</td>\n",
       "      <td>0.554421</td>\n",
       "      <td>0.595022</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.983897</td>\n",
       "      <td>0.767162</td>\n",
       "      <td>1.139465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.778261</td>\n",
       "      <td>0.675639</td>\n",
       "      <td>0.848432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marriage</th>\n",
       "      <td>1.544769</td>\n",
       "      <td>1.026339</td>\n",
       "      <td>0.984819</td>\n",
       "      <td>1.265035</td>\n",
       "      <td>0.883983</td>\n",
       "      <td>0.966650</td>\n",
       "      <td>1.049365</td>\n",
       "      <td>0.612590</td>\n",
       "      <td>0.657744</td>\n",
       "      <td>0.655153</td>\n",
       "      <td>0.767250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.797377</td>\n",
       "      <td>0.867583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penance</th>\n",
       "      <td>1.537118</td>\n",
       "      <td>1.447291</td>\n",
       "      <td>0.747760</td>\n",
       "      <td>1.400470</td>\n",
       "      <td>0.902367</td>\n",
       "      <td>0.878096</td>\n",
       "      <td>1.307722</td>\n",
       "      <td>0.915197</td>\n",
       "      <td>0.810059</td>\n",
       "      <td>1.099163</td>\n",
       "      <td>0.760864</td>\n",
       "      <td>0.814613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>1.373990</td>\n",
       "      <td>1.085234</td>\n",
       "      <td>0.776417</td>\n",
       "      <td>1.171744</td>\n",
       "      <td>1.062325</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>0.797089</td>\n",
       "      <td>1.067428</td>\n",
       "      <td>0.786108</td>\n",
       "      <td>1.240770</td>\n",
       "      <td>0.877029</td>\n",
       "      <td>0.879577</td>\n",
       "      <td>0.892687</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cases      laws   orders1   orders2    simony  procedure  \\\n",
       "cases           NaN  2.276541  1.924653  2.025227  1.963736   1.954481   \n",
       "laws       2.140962       NaN  1.248994  1.502025  1.463297   1.314724   \n",
       "orders1    1.618391  1.094915       NaN  1.122318  0.968477   0.884251   \n",
       "orders2    1.898201  1.524410  1.268620       NaN  1.382037   1.683955   \n",
       "simony     1.666670  1.349079  0.977183  1.219516       NaN   0.887776   \n",
       "procedure  1.618734  1.199074  0.892039  1.509545  0.878923        NaN   \n",
       "other1     1.335318  1.300002  1.061945  1.272178  1.138310   1.075322   \n",
       "other2     1.941600  1.323324  1.091294  1.629055  1.138646   1.109048   \n",
       "monastic   1.455489  1.045089  0.855446  1.267636  1.011384   0.798639   \n",
       "other3     2.070457  1.338796  1.288958  1.514642  1.199653   1.105702   \n",
       "heresy     1.517690  1.030980  0.777157  1.218226  0.554421   0.595022   \n",
       "marriage   1.544769  1.026339  0.984819  1.265035  0.883983   0.966650   \n",
       "penance    1.537118  1.447291  0.747760  1.400470  0.902367   0.878096   \n",
       "second     1.373990  1.085234  0.776417  1.171744  1.062325   0.963429   \n",
       "\n",
       "             other1    other2  monastic    other3    heresy  marriage  \\\n",
       "cases      1.571380  2.278196  1.762169  2.362842  1.871684  1.892278   \n",
       "laws       1.422318  1.436900  1.193099  1.434469  1.187540  1.192357   \n",
       "orders1    1.049854  1.110873  0.869308  1.239660  0.826694  1.012354   \n",
       "orders2    1.414852  1.687281  1.449174  1.620788  1.419841  1.452577   \n",
       "simony     1.130389  1.128661  1.041277  1.171096  0.590034  0.916648   \n",
       "procedure  1.079020  1.122308  0.820975  1.072558  0.656892  0.999328   \n",
       "other1          NaN  1.279177  0.964928  1.305428  0.996004  1.085321   \n",
       "other2     1.296338       NaN  0.797917  1.034580  1.059174  0.653951   \n",
       "monastic   0.930010  0.742869       NaN  1.057799  0.760244  0.661069   \n",
       "other3     1.349715  0.950503  1.122944       NaN  1.120868  0.712106   \n",
       "heresy     0.948498  0.983897  0.767162  1.139465       NaN  0.778261   \n",
       "marriage   1.049365  0.612590  0.657744  0.655153  0.767250       NaN   \n",
       "penance    1.307722  0.915197  0.810059  1.099163  0.760864  0.814613   \n",
       "second     0.797089  1.067428  0.786108  1.240770  0.877029  0.879577   \n",
       "\n",
       "            penance    second  \n",
       "cases      1.858944  1.633416  \n",
       "laws       1.621796  1.232300  \n",
       "orders1    0.750538  0.777653  \n",
       "orders2    1.552287  1.319530  \n",
       "simony     0.905909  1.086345  \n",
       "procedure  0.881784  0.985186  \n",
       "other1     1.327212  0.815153  \n",
       "other2     0.863307  1.096104  \n",
       "monastic   0.799881  0.779872  \n",
       "other3     1.152123  1.306677  \n",
       "heresy     0.675639  0.848432  \n",
       "marriage   0.797377  0.867583  \n",
       "penance         NaN  0.902606  \n",
       "second     0.892687       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './corpus/b/'\n",
    "\n",
    "candidates = ['cases', 'laws', 'orders1', 'orders2', 'simony', 'procedure', 'other1', 'other2', 'monastic', 'other3', 'heresy', 'marriage', 'penance', 'second']\n",
    "deltas = pd.DataFrame(columns = candidates)\n",
    "limit = 30 # 30 most frequent words (MFWs)\n",
    "for candidate in candidates:\n",
    "    unknown = candidate\n",
    "    samples = candidates[:]\n",
    "    samples.remove(unknown)\n",
    "    features = get_features(samples)\n",
    "    mfws = list(features.keys())[:limit]\n",
    "    counts = get_counts(mfws, [unknown] + samples)\n",
    "    lengths = get_lengths([unknown] + samples)\n",
    "    frequencies = (counts / lengths.values) * 1000\n",
    "    means = frequencies[samples].mean(axis = 1).to_frame('mean')\n",
    "    standard_deviations = frequencies[samples].std(axis = 1).to_frame('std')\n",
    "    z_scores = (frequencies - means.values) / standard_deviations.values\n",
    "    test = z_scores[[unknown]]\n",
    "    corpus = z_scores[samples]\n",
    "    differences = (test.values - corpus).abs()\n",
    "    row = (differences.mean(axis = 0)).to_frame(unknown).transpose()\n",
    "    deltas = deltas.append(row)\n",
    "deltas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
